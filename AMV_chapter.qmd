---
title: "AMV_chapter"
format: html
editor: visual
---

Les données du biologiste, qu'il s'intéresse aux individus, aux populations ou aux communautés, se présentent le plus souvent sous forme de tableaux rectangulaires croisant des observations (n) en lignes et des variables (p) en colonnes.

La représentation graphique de ces n observations et p variables est facilement réalisable lorsque l'on a seulement 2 ou 3 variables (dimension). Cependant, quand le nombre de variables augmente la représentation graphique devient compliquée et l'utilisation des Analyses Multivariées prend tout son sens !

L'objectif de ces analyses est de réduire le nombre de dimensions et d'examiner la structure des données en répondant aux questions suivantes :

-   Quels observations se ressemblent ?

-   Y-a-t-il des observations qui se distinguent ? Des sous-groupes ?

-   Quelles sont les variables corrélées entre elles ?

-   Y-a-t-il des liens particulier entre certains observations et/ou variables ?

Les méthodes d'analyse multivariées servent donc à décrire les données et à émettre des hypothèses pouvant être testées par la suite.

# PART 1: PCA

## Introduction :

L'ACP permet de traiter un tableau de mesure avec :

-   En ligne : les **n observations**

-   En colonne : le **p variables quantitatives**

Les 2 ensemble (observations et variables) sont **nettement distincts et non interchangeable**. C'est-à-dire que si l'on interchangent les lignes et les colonnes (en lignes les variables et en colonnes les observations), le tableau n'a plus le même sens.

Dans les tableaux traiter par l'ACP, la moyenne d'une colonne à un sens alors que la moyenne d'une ligne en n'a pas.

L'objectif de l'ACP est de réaliser la meilleure représentation géométrique des individus et des variables. Pour cela on cherche à réduire la dimension en trouvant le meilleur plan (sous-espace) de projection permettant de visualiser « au mieux » le nuage de points en espace réduits.

Cependant, cette réduction doit :

-   Préserver les distances entre individus :
    -   Deux individus qui se ressemblent doivent être proche dans l'espace de représentation.
-   Préserver les corrélations entre les variables :
    -   Deux variables qui sont corrélées doivent être représentées par des vecteurs formant un angle aigu.
    -   Deux variables indépendantes sont représentées par des vecteurs orthogonaux.

## Mathematics

## Interpretation

Voici un exemple d'application de l'ACP. *presentation jeu de donnee* *package necessaire*

### a) Importation du jeu de donnees :

```{r}
# Chargement du jeu de donnees "iris" disponible sur R :
data("decathlon2") 
data=decathlon2[decathlon2$Competition=="OlympicG",c(1:10,12)]
summary(data)
```

Dans ce jeu de donnees, on a donc 14 individus (athletes) sur lesquels ont ete releve 10 variables quantitatives (performances dans differentes disciplines sportives) : - X100m - Long.jump - Shot.put - X400m - X110m.hurdle - Discus - Pole.vault - Javeline - X1500m

Si on a la presence de NAs dans les donnees. Afin de pouvoir realiser l'ACP sans encombre avec les NAs, on peut les ommetre de l'analyse avec la commande suivante :

```{r}
# data=na.omit(object = data)
```

On cherche a savoir si dans ce jeu de donnees des liens particulier entre certaines observations et/ou variables peuvent etre observes ?

### b) Etude des correlations :

Obtention de la matrice des correlations entre les variables en utilisant la fonction **cor()** :

```{r}
cor(data)
```

Les variables sont correlees si leur valeur est superieur a 0.9 (en regle general) On peut voir ici, qu'aucune variable sont correlees.

Voici une autre facon de visualiser les correlations entre variable : (utiles lorsque l'on a beaucoup de variables comme ici):

```{r}
abs(cor(data))>0.9
```

Si TRUE (en dehors de la diagonale), alors les deux variables sont correlees. Si deux variables sont correlees, il faut en supprimer une. Le choix de l'elimination d'une des deux variables correlees est arbitraire et depend de la question posee.

Pour supprimer une variable du jeu de donnees (data), on peut utiliser la fonction suivante : **data=data\[,-(colonne_de_la_variable_a_enlever)\]**

### c) Réalisation de l'ACP :

Afin de realiser l'ACP, il est necessaire de charger les deux packages suivants : - **factoextra** - **FactoMineR**

Ensuite pour realiser l'ACP sur R, on peut utiliser la fonction : **PCA()**. Il faut penser a stocker le resultat de cette ACP dans une nouvelle variable afin de pouvoir recuperer facilement les informations de l'ACP que l'on a besoin par la suite pour l'interpretation.

```{r, echo=FALSE, hide=TRUE}
library(FactoMineR)
library(factoextra)
PCA=PCA(data)
```

### d) Interpretation des sorties :

#### 1) Inertie et choix des axes :

```{r}
# Recuperer les valeurs propres de chaque axes et l'inerties portees par les composantes principales
PCA$eig

# Visualisation graphique de l'inertie de chaque axe : 
fviz_eig(PCA, addlabels = TRUE)
```

Pour savoir le nombre d'axe que l'on prend pour l'analyse de l'ACP, il faut reperer le saut dans la variance expliquee par les differents axes. Ici on peut voir, que le premier axe represente 43.2% de la variance, le second axe 22%, le troisieme axe 14.1%, ... On peut donc voir que le saut dans la variance expliquee par les differents axes s'effectue entre le premier axe et le deuxieme. La difference entre la variance expliquee par les autres axes que le premier est negligeable par rapport a la difference entre le premier axe et les autres.

On garde donc pour l'interpretation de la PCA, les deux premiers axes qui explique a eux deux 65.19% de la variance.

#### 2) Interpretation du sens biologique des axes :

En ACP, l'interpretion des axes se fait selon les colonnes (\<=\> variables) via le cercle des correlations et les contributions absolues des variables.

Pour obtenir ces deux informations sur R, on peut utiliser les commandes suivantes :

```{r}
# Obtention des contributions absolues des variables :
PCA$var$contrib

# Obtention du cercle des correlations :
fviz_pca_var(PCA)

# Representation graphique des valeurs de contributions absolues des variables pour un axe : 
# Cas axe 1 : 
fviz_contrib(PCA, choice = "var", axes = 1)

# Cas axe 2: 
fviz_contrib(PCA, choice = "var", axes = 2)
```

La contribution absolue permet de savoir comment la variable initiale contribue a la formation de l'axe. Pour savoir a partir de ces resultats, quelles variables contribuent a la formation de l'axe synthetique, on utilise le seuil de \*\*1/nombre_de_variables)\*100\*\* (en general). Pour que la variable contribue, il faut que sa valeur de contribution soit superieure a ce seuil. Cette valeur de seuil correspond a la ligne en pointille rouge sur les graphiques presentant les contributions absolues des variables selon les axes.

D'apres ces resultats, on peut voir que l'axe 1 est represente principalement par les variables : - Points - Discus - Shotput - Long.jump - X100m - High.jump

L'axe 2 est lui represente par les variables : - X1500m - Pole.vault - X110m.hurdle - X100m

Par ailleurs, le cercle des correlations nous permet de voir que certaines variables sont liees et d'autres independantes. En effet, deux variables qui sont liees sont representees par des vecteurs formant un angle aigu. Deux variables indépendantes sont représentées par des vecteurs orthogonaux.

#### 3) Projection des individus afin d'en effectuer une typologie sur la base des axes :

Pour cela on utilise les contributions relatives des individus. En effet, ces contributions relatives permettent d'effectuer une typologie des individus sur la base des axes. La contribution relative d'un individu correspond à la part d'information de l'axe explique par le point et pris en charge sur l'axe (en pourcentage).

Pour obtenir ces contributions relatives des individus sur R, on utilise les commandes suivantes :

```{r}
# Obtention des contributions relatives des individus :
PCA$ind$cos2

# Obtention de la projection des individus : 
fviz_pca_ind(PCA)



```

Ainsi les individus qui explique le plus l'axe 1 sont : - Sebrle - Clay - Karpov - Schwarzl - Drews - Schoenbeck

Ceux qui explique le plus l'axe 2 sont : - Macey - Warners - Zsivoczky - Barras

### e) Conclusion biologique :

Ainsi, grace aux resultats des contributions absolues des variables et des contributions relatives des individus, on peut representer la projection des individus et des variables ainsi :

```{r}
fviz_pca_biplot(PCA)
```

Sachant l'interpretation biologique des axes : *IMAGE*

On peut donc voir que les individus ayant le plus grand score a ce tournoi, sont ceux qui ont eu les meilleurs resultats au epreuves : discus, shot put, long jump et high jump. De plus, avoir de bons resultats dans la course de 100m ne permet pas d'avoir un bon classement.

Grace a la PCA, on peut voir que le score final est plus lie au score de certaines disciplines que d'autres. Et donc, que des liens entre certaines variables et individus peuvent etre observes.

A citation @bauer2023writing

# PART 2: CA

## Mathematiques

## Interpretation

This chapter is a simple example using R

You can import R package using the code

```{r}
library(tidyverse)
```

and then describe the purpose of your chapter as well as executing R command.

For example a basic summary of a dataset is given by

```{r}
df <- read.table("https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv", sep = "," , header = TRUE)
```

and produce a graph

```{r}
df %>% ggplot() +
	aes(x=species, y = body_mass_g) +
	geom_boxplot()  
```

A citation @bauer2023writing

# PART 3: RDA

## MathÃ©matiques

## InterprÃ©tation

This chapter is a simple example using R

You can import R package using the code

```{r}
library(tidyverse)
```

and then describe the purpose of your chapter as well as executing R command.

For example a basic summary of a dataset is given by

```{r}
df <- read.table("https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv", sep = "," , header = TRUE)
```

and produce a graph

```{r}
df %>% ggplot() +
	aes(x=species, y = body_mass_g) +
	geom_boxplot()  
```

A citation @bauer2023writing

# PART 4: RLQ

## MathÃ©matiques

## InterprÃ©tation

The use of RLQ analysis is important in ecology to integrate the traits of species with the environmental variables. So here, we don't have 2 tables (environment & specie) as RDA part but 3 tables:

-   environmental variables by sites (R)

-   abundance of species by sites (L)

-   trait values by species (Q)

To perform the RLQ, we need to decompose the analyse by three type of analyses already done in this chapter:

-   we will use a CA analyse on the abundance of species

-   we will use a MCA on the environmental table by taking the sites weight on the CA

-   we will use a MCA on the trait table by taking the species weight on the CA

Here, we use MCA for R and Q because our variables are factors but you can perform a PCA if your variables are quantitatives. Warning, for R and Q you have the obligation to weight by the L table (see below).

You can import R package using the code

```{r}
library(tidyverse)
library(ade4)
library(vegan)
library(ggplot2)
library(factoextra)
library(corrplot)
library(RVAideMemoire)
library(PerformanceAnalytics)
```

Here, we work with a dataset of "*ade4*" package

```{r}
#import the dataset
data(aviurba)

#create the three tables
summary(aviurba$mil)    #(R)
R<-aviurba$mil

summary(aviurba$fau)    #(L)
L<-aviurba$fau

summary(aviurba$traits) #(Q)
Q<-aviurba$traits
```

and explore the tables

```{r}
head(R)  
head(L)
head(Q)
```

The first part is to perform our CA on specie table

```{r}
afcL <- dudi.coa(log(L+1), scannf = FALSE) 
afcL  

```

The first CA is done. We use log transformation because the abundance of species has a large range and we add "+1" to avoid the log(0) for some species.

Now, we can perform the two MCA analysis on the trait table and environmental table

```{r}
acmR <- dudi.acm(R, row.w = afcL$lw, scannf = FALSE,nf = 4)
scatter(acmR)

acmQ <- dudi.acm(Q, row.w = afcL$cw, scannf = FALSE,nf = 4)
scatter(acmQ)

```

The scatterplot allows to see the ordination of each table and the repartition of factor on the simple axe of MCA.

But now, we will use the RLQ analyse that creates three co-inertia (R-L, L-Q, R-Q), assembles and compares the co-inertia. We use the rlq function for that.

```{r}
rlq <- rlq(acmR, afcL, acmQ, scannf = FALSE)
rlq
axe=c(1:8)
print(paste("The contribution of axe nÂ°",axe, "are", rlq$eig/(sum(rlq$eig))*100,"%"))
#randtest(rlq)
#summary(rlq)
#plot(rlq)

```

Here, the output of the RLQ is complex but only few information are, at this point important. We see that we have 8 eigenvalues and we have their values. All the differents compounds of the output will be used after in representations or analysis.

Nevertheless, we can calculate the contribution of each axis of the RLQ by performing the formule below: METTRE LA FORMULE AU PROPRE rlq$eig/(sum(rlq$eig))\*100

After that, and before to plot and analyse the result, it is important to test if the result of the RLQ is not only due to random combination of values but that we have a real correlation between are different tables. To produce this, we perform a permutation test with the function *randtest*.

```{r}
randtest(rlq)
plot(randtest(rlq))


```

The outputs above corresponds to the permutation test. We see that the number of permutation of columns and rows was to 999 (default value).

The results of this test shows that the permutation of sites (rows) is the first result (p_value=0.1%) and the permutation of species (columns) is the second result (p_value=1.9%) So each result is significant (5% threshold) and we can conclude that our RLQ result is not linked to random effect.

Finally, the results of the RLQ are plot in the distribution law calculated by the permutation.

A citation @bauer2023writing
